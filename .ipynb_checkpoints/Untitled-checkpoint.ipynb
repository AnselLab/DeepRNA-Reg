{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0681c4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad56f380",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########      ##########    ##########  ##########              ##########  ####        ####   ############\n",
    "############    ##########    ##########  ####   ####             ##########  ####        ####   ####     ####\n",
    "##############  ####          ####        ####    ####            ####        ####        ####   ####      ####\n",
    "####      ####  ####          ####        ####    ####            ####        ####        ####   ####      ####\n",
    "####      ####  ##########    ##########  ####   ####             ####        ####        ####   ####     ####\n",
    "####      ####  ##########    ##########  ##########              ####        ####        ####   ############\n",
    "####      ####  ####          ####        ####                    ####        ####        ####   ####\n",
    "##############  ####          ####        ####                    ####        ####        ####   ####\n",
    "############    ##########    ##########  ####                    ##########  ##########  ####   ####\n",
    "##########      ##########    ##########  ####                    ##########  ##########  ####   ####\n",
    "\n",
    "\n",
    "\n",
    "###### Import Dependencies\n",
    "\n",
    "import numpy as np\n",
    "from datascience import *\n",
    "from scipy.integrate import simps\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.stats import percentileofscore\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def DEEP_CLIP(loci):\n",
    "    #import shutup; shutup.please()\n",
    "    \n",
    "    #from keras.models import Sequential, load_model\n",
    "    #from keras.layers import Dense, LSTM, TimeDistributed, Activation\n",
    "    #from keras import optimizers\n",
    "    from keras.models import load_model\n",
    "    \n",
    "    pbar = tqdm.tqdm(total=len(loci))\n",
    "    \n",
    "    \n",
    "\n",
    "    def get_depth_data(track_files,track_names,chrom,start,stop,strand,track_type):\n",
    "        def view_region(track_file,strand,region):\n",
    "            return subprocess.Popen((\"samtools\", \"view\",\n",
    "                                     strand_to_flag[use_strand],\n",
    "                                     \"-b\",track_file,\n",
    "                                     region),stderr=subprocess.PIPE,stdout=subprocess.PIPE)\n",
    "        mydepths = pd.DataFrame([0]*(stop-start+1),index=range(start,stop+1),columns=[\"depth\"])\n",
    "        depth_list = pd.DataFrame(0,index=range(start,stop),columns=track_names)\n",
    "        strandinvert = {\"+\":\"-\",\"-\":\"+\"}\n",
    "        strand_to_flag = {\"+\":\"-F 0x10\",\n",
    "            \"-\":\"-f 0x10\"}\n",
    "        for n,track_file in enumerate(track_files):\n",
    "            use_strand=strand\n",
    "            region = chrom + \":\" + str(start) + \"-\" + str(stop)\n",
    "            if track_type[n] == \"as\":\n",
    "                use_strand = strandinvert[strand]\n",
    "            # Get sequences from a given region (in binary bam format still)\n",
    "            ps =view_region(track_file,strand_to_flag[use_strand],region)\n",
    "            sout,err = ps.communicate() # get stdout, stderr\n",
    "            ## CHECK TO MAKE SURE THE REFERENCE GENOME CHROMOSOME IS FINE.\n",
    "            if len(err)>0: # is there anytihn in stder?\n",
    "                if b\"specifies an unknown reference name\" in err:\n",
    "                    # SWITCH REFERENCE\n",
    "                    temp_chrom = chrom.replace(\"chr\",\"\")\n",
    "                    region = temp_chrom + \":\" + str(start) + \"-\" + str(stop)\n",
    "                    ps =view_region(track_file,strand_to_flag[use_strand],region)\n",
    "                    sout,err = ps.communicate()\n",
    "            if len(err)>0:\n",
    "                raise NameError(\"Unknown samtools error. Ran: samtools view %s -b %s %s | samtools depth - \" % (strand_to_flag[use_strand],track_file,region))\n",
    "            # Run samtools depth on the sequences retrieved\n",
    "            ps2 = subprocess.Popen((\"samtools\", \"depth\",\"-\"),stdin=subprocess.PIPE,stdout=subprocess.PIPE)\n",
    "            output,err = ps2.communicate(input=sout)\n",
    "            sample_depths = pd.read_table(BytesIO(output),names=[\"chrom\",\"depth\"],index_col=1)\n",
    "            if len(sample_depths.index)>0:\n",
    "                mydepths.depth = sample_depths.depth\n",
    "                depth_list[track_names[n]] = sample_depths.depth\n",
    "                depth_list = depth_list.fillna(value=0)  \n",
    "        return depth_list\n",
    "    def ranges(nums):\n",
    "        nums = sorted(set(nums))\n",
    "        gaps = [[s, e] for s, e in zip(nums, nums[1:]) if s+1 < e]\n",
    "        edges = iter(nums[:1] + sum(gaps, []) + nums[-1:])\n",
    "        summ = list(zip(edges, edges))\n",
    "        start = [summ[x][0] for x in np.arange(len(summ))]\n",
    "        stop = [summ[x][1] for x in np.arange(len(summ))]\n",
    "\n",
    "        return Table().with_columns(\"start\", start, \"stop\", stop, \"range\", [i-j for i,j in zip(stop,start)])\n",
    "\n",
    "    def range_generator(summary_tbl):\n",
    "\n",
    "        cond_1 = ranges(summary_tbl.where(\"Greater Binding in Condition 1\", are.equal_to(True)).column(0))\n",
    "        cond_2 = ranges(summary_tbl.where(\"Greater Binding in Condition 2\", are.equal_to(True)).column(0))\n",
    "        return cond_1.where(\"range\", are.above(3)), cond_2.where(\"range\", are.above(3)), summary_tbl\n",
    "\n",
    "    ### We are throwing away any regions called that have a lenth lower than 4 nucleotides\n",
    "\n",
    "    \n",
    "    def MA_normalization(read_data):\n",
    "        M = np.log(read_data.column(1) + .999999) - np.log(read_data.column(2) + .999999)\n",
    "        A = (np.log(read_data.column(1) + .999999) + np.log(read_data.column(2) + .999999))\n",
    "        slope, intercept, r_value, p_value, std_err =  stats.linregress(A,M)\n",
    "        adjusted_M = M - ((slope*A)+intercept)\n",
    "\n",
    "        return adjusted_M\n",
    "\n",
    "\n",
    "    model = load_model(model_name)\n",
    "\n",
    "\n",
    "    def get_pred(arr):\n",
    "        a = ((arr- np.mean(arr))/np.std(arr))\n",
    "        b = ((arr- np.mean(arr))/np.std(arr))\n",
    "        seq = [[x,y] for x,y in zip(a,b)]\n",
    "        seq = np.array(seq)\n",
    "        X, y = seq[:, 0], seq[:, 1]\n",
    "        X = X.reshape((len(X), 1, 1))\n",
    "        return X, y\n",
    "\n",
    "\n",
    "    def auc_generator(condition, summ_tbl):\n",
    "        depths = []\n",
    "        startZ = condition.column(\"start\")\n",
    "        stopZ = condition.column(\"stop\")\n",
    "        for i in np.arange(condition.num_rows):\n",
    "            start = startZ[i]\n",
    "            stop = stopZ[i]\n",
    "            depth = simps(summ_tbl.where(0, are.between(start, stop+1)).column(\"Adjusted_Fitted_M\"))\n",
    "            if depth <0:\n",
    "                depth = -1*depth # We multiply by -1 because we don't care wether integrated area is positive or negative\n",
    "                                # We only care for the intensity (magnitude) of the integral\n",
    "            depths.append(depth)\n",
    "        return np.asarray(depths)\n",
    "    \n",
    "    cond1_starts, cond1_stops, cond1_strands, cond1_chroms, cond1_geneIDs, cond1_depths = [], [], [], [], [], []\n",
    "    cond2_starts, cond2_stops, cond2_strands, cond2_chroms, cond2_geneIDs, gene_runs, cond2_depths = [], [], [], [], [], [], []\n",
    "\n",
    "\n",
    "    for locus in loci:\n",
    "        pbar.update(1)\n",
    "        chrom = locus[0] \n",
    "        start = int(locus[1])\n",
    "        stop = int(locus[2])\n",
    "        geneid = locus[3]\n",
    "        strand = locus[5]\n",
    "        region = chrom + \":\" + str(start) + \"-\" + str(stop)\n",
    "        \n",
    "        depths = get_depth_data(make_array(f1, f2),make_array(f1, f2),chrom,start,stop,strand,[\"s\", \"s\"])\n",
    "        f1_depths = depths[f1].tolist()\n",
    "        f2_depths = depths[f2].tolist()\n",
    "\n",
    "        reading_data = Table().with_columns(\"Unnamed: 0\",depths.index.tolist(), f1,f1_depths, f2, f2_depths)\n",
    "\n",
    "        \n",
    "        if sum(reading_data.column(1)) > 0 and sum(reading_data.column(2)) > 0 and reading_data.num_rows >21:\n",
    "            difference = reading_data.column(2) - reading_data.column(1)\n",
    "            if sum(difference  == difference[0]) != len(difference): #There cannot be a constant difference in read depth\n",
    "                if sum(np.isnan(MA_normalization(reading_data))) != len(reading_data.column(1)):\n",
    "                    predictor = savgol_filter(MA_normalization(reading_data), 21,3)\n",
    "                    reading_data = reading_data.with_column(\"m_val\", predictor )#.select(0,3)\n",
    "                    vals = []\n",
    "                    X,y = get_pred(predictor)\n",
    "                    yhat = model.predict([X], verbose=0)\n",
    "\n",
    "                    if sum(np.isnan(yhat))[0] != len(yhat):\n",
    "                        # If we get an array of all NANs, ignore it.\n",
    "                        yhat = savgol_filter([yhat[z][0] for z in np.arange(len(yhat))], 21,1)\n",
    "                        for b in np.arange(len(yhat)):\n",
    "                            val = yhat[b]\n",
    "                            min_pos = np.argmin([abs(1.-val), abs(val), abs(-1-val)])\n",
    "                            if min_pos ==0:\n",
    "                                val = 1\n",
    "                            if min_pos ==1:\n",
    "                                val = 0\n",
    "                            if min_pos ==2:\n",
    "                                val = -1\n",
    "                            vals.append(val)\n",
    "                        reading_data = reading_data.with_columns(\"vals\", vals)\n",
    "                        ##PLOT DATA TEST\n",
    "                        ##reading_data.plot(0,width=15, height=7)\n",
    "                        reading_data = reading_data.with_column(\"Greater Binding in Condition 1\",reading_data.column(\"vals\")==1 )\n",
    "                        reading_data = reading_data.with_column(\"Greater Binding in Condition 2\",reading_data.column(\"vals\")==-1 )\n",
    "                        tbl = reading_data.relabeled(\"Unnamed: 0\", \"pos\").relabeled(\"m_val\", \"Adjusted_Fitted_M\").relabeled(\"vals\", \"lstm predictions\")\n",
    "                        cond1, cond2, summ_tbl = range_generator(tbl)\n",
    "                        cond1 = cond1.with_column(\"strand\", [strand for i in cond1.column(0)]).with_column(\"chrom\", [chrom for i in cond1.column(0)]).with_column(\"geneid\", [geneid for z in cond1.column(0)])\n",
    "                        cond2 = cond2.with_column(\"strand\", [strand for i in cond2.column(0)]).with_column(\"chrom\", [chrom for i in cond2.column(0)]).with_column(\"geneid\", [geneid for z in cond2.column(0)])\n",
    "\n",
    "                        [cond1_geneIDs.append(i) for i in cond1.column(\"geneid\")]\n",
    "                        [cond1_chroms.append(i) for i in cond1.column(\"chrom\")]\n",
    "                        [cond1_starts.append(i) for i in [int(y) for y in cond1.column(\"start\")]]\n",
    "                        [cond1_stops.append(i) for i in [int(y) for y in cond1.column(\"stop\")]]\n",
    "                        [cond1_strands.append(i) for i in cond1.column(\"strand\")]\n",
    "                        [cond1_depths.append(i) for i in auc_generator(cond1, summ_tbl)]\n",
    "\n",
    "                        [cond2_geneIDs.append(i) for i in cond2.column(\"geneid\")]\n",
    "                        [cond2_chroms.append(i) for i in cond2.column(\"chrom\")]\n",
    "                        [cond2_starts.append(i ) for i in [int(y) for y in cond2.column(\"start\")]]\n",
    "                        [cond2_stops.append(i) for i in [int(y) for y in cond2.column(\"stop\")]]\n",
    "                        [cond2_strands.append(i) for i in cond2.column(\"strand\")]\n",
    "                        [cond2_depths.append(i) for i in auc_generator(cond2, summ_tbl)]\n",
    "\n",
    "\n",
    "    condition_1 = Table().with_columns(\"geneid\", cond1_geneIDs, \"chrom\",cond1_chroms, \"start\",[int(y) for y in cond1_starts], \"stop\", [int(y) for y in cond1_stops],\"strands\",cond1_strands, \"AUC Differential Binding\",cond1_depths )\n",
    "    condition_2 = Table().with_columns(\"geneid\", cond2_geneIDs, \"chrom\",cond2_chroms, \"start\",[int(y) for y in cond2_starts], \"stop\", [int(y) for y in cond2_stops],\"strands\",cond2_strands, \"AUC Differential Binding\", cond2_depths)\n",
    "    \n",
    "    return [condition_1.to_df(),condition_2.to_df()]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "058e0d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "bed_filename = 'mm10_3pUTRs_curated_merged.txt'\n",
    "f1 = \"bams/miR29_KO_Th17_collapsed.bam\"\n",
    "f2 = \"bams/miR29_WT_Th17_collapsed.bam\"\n",
    "model_name = 'DeepRNAreg.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5fe62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b4a2647",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                          | 1/300 [00:00<02:50,  1.76it/s]2023-01-19 21:28:06.265794: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-01-19 21:28:06.265814: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-01-19 21:28:06.265809: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-01-19 21:28:06.265832: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-01-19 21:28:06.265810: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-01-19 21:28:06.265806: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-01-19 21:28:06.265875: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-01-19 21:28:06.268319: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-01-19 21:28:06.273933: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-01-19 21:28:06.278732: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "100%|█████████████████████████████████████████| 300/300 [00:49<00:00,  6.02it/s]\n",
      "100%|█████████████████████████████████████████| 300/300 [00:53<00:00,  5.63it/s]\n",
      " 86%|███████████████████████████████████▎     | 258/300 [00:57<00:07,  5.73it/s]\n",
      "100%|█████████████████████████████████████████| 300/300 [00:57<00:00,  5.18it/s]\n",
      "100%|█████████████████████████████████████████| 300/300 [00:58<00:00,  5.10it/s]\n",
      "100%|█████████████████████████████████████████| 300/300 [00:59<00:00,  5.04it/s]\n",
      "100%|█████████████████████████████████████████| 300/300 [01:01<00:00,  4.91it/s]\n",
      "100%|█████████████████████████████████████████| 300/300 [01:02<00:00,  4.77it/s]\n",
      "100%|█████████████████████████████████████████| 300/300 [01:05<00:00,  4.61it/s]\n",
      "100%|█████████████████████████████████████████| 300/300 [01:06<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Runtime: 69.2638669013977 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shaansekhon/.pyenv/versions/3.9.7/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n",
      "/Users/shaansekhon/.pyenv/versions/3.9.7/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n",
      "/Users/shaansekhon/.pyenv/versions/3.9.7/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n",
      "/Users/shaansekhon/.pyenv/versions/3.9.7/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n",
      "/Users/shaansekhon/.pyenv/versions/3.9.7/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n",
      "/Users/shaansekhon/.pyenv/versions/3.9.7/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n",
      "/Users/shaansekhon/.pyenv/versions/3.9.7/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n",
      "/Users/shaansekhon/.pyenv/versions/3.9.7/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n",
      "/Users/shaansekhon/.pyenv/versions/3.9.7/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n",
      "/Users/shaansekhon/.pyenv/versions/3.9.7/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import multiprocess\n",
    "from multiprocess import Pool\n",
    "import tqdm\n",
    "import subprocess\n",
    "from math import log\n",
    "from io import BytesIO\n",
    "\n",
    "start_time = time.time()\n",
    "with Pool(processes=multiprocess.cpu_count(),maxtasksperchild=20) as pool:\n",
    "    results = pool.map(DEEP_CLIP,  )\n",
    "stop_time = time.time()\n",
    "\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "print(\"--- Runtime: %s seconds ---\" % (stop_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd2e0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [[Table.from_df(j) for j in i] for i in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9a3fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31eac6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_C_list(cond_2,DBE_col):\n",
    "    arr = cond_2.column(DBE_col)\n",
    "\n",
    "    # pre-sort array\n",
    "    arr_sorted =  sorted(arr)\n",
    "\n",
    "    # calculate percentiles using scipy func percentileofscore on each array element\n",
    "    s = pd.Series(arr)\n",
    "    percentiles = s.apply(lambda x: percentileofscore(arr_sorted, x))\n",
    "    def quality_score(pct):\n",
    "        if pct <= 25:\n",
    "            return 1\n",
    "        if pct <= 50:\n",
    "            return 2\n",
    "        if pct <=70:\n",
    "            return 3\n",
    "        if pct <= 80:\n",
    "            return 4\n",
    "        if pct <= 85:\n",
    "            return 5 \n",
    "        if pct <= 90:\n",
    "            return 6 \n",
    "        if pct <= 95:\n",
    "            return 7\n",
    "        if pct <= 97.5:\n",
    "            return 8 \n",
    "        if pct <= 99:\n",
    "            return 9\n",
    "        if pct <= 100:\n",
    "            return 10\n",
    "    cond_2 = cond_2.with_columns(\"DBE Percentile\", percentiles, \"DBE Score\", [quality_score(p) for p in percentiles])\n",
    "    return cond_2\n",
    "\n",
    "def concat_files(TABLEZ):\n",
    "    starts, stops, strands, chroms, geneIDs, depths = [], [], [], [], [], []\n",
    "    for tbl in TABLEZ:\n",
    "        [starts.append(i) for i in tbl.column(\"start\")]\n",
    "        [stops.append(i) for i in tbl.column(\"stop\")]\n",
    "        [strands.append(i) for i in tbl.column(\"strands\")]\n",
    "        [chroms.append(i) for i in tbl.column(\"chrom\")]\n",
    "        [geneIDs.append(i) for i in tbl.column(\"geneid\")]\n",
    "        [depths.append(i) for i in tbl.column(\"AUC Differential Binding\")]\n",
    "    \n",
    "        \n",
    "    tbl = Table().with_columns(\"chrom\", chroms,\"start\", starts, \"stop\", stops,\"geneid\",geneIDs,\"Differential Binding Enrichment(DBE)\",depths, \"strand\", strands )\n",
    "    tbl = q_C_list(tbl, 'Differential Binding Enrichment(DBE)')\n",
    "    return tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d83aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond1 =concat_files([i[0] for i in results])\n",
    "cond2 =concat_files([i[1] for i in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54257e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond1.to_csv('enriched_cond1.csv')\n",
    "cond2.to_csv('enriched_cond2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2e1a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
